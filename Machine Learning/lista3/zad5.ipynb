{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.special import expit  # Sigmoid function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dataset = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(1, 21)])\n",
    "dataset['Target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue=\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, regularization=None, lambda_reg=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            predictions = expit(X @ self.coef_)\n",
    "            gradient = X.T @ (y - predictions)\n",
    "\n",
    "            if self.regularization == 'L1':\n",
    "                gradient -= self.lambda_reg * np.sign(self.coef_)\n",
    "            elif self.regularization == 'L2':\n",
    "                gradient -= 2 * self.lambda_reg * self.coef_\n",
    "\n",
    "            self.coef_ += self.learning_rate * gradient\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return expit(X @ self.coef_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns='Target')\n",
    "y = dataset['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularization\n",
    "log_reg_l2 = LogisticRegression(regularization='L2')\n",
    "log_reg_l2.fit(X_train, y_train)\n",
    "y_pred_l2 = log_reg_l2.predict(X_test)\n",
    "\n",
    "# L1 Regularization\n",
    "log_reg_l1 = LogisticRegression(regularization='L1')\n",
    "log_reg_l1.fit(X_train, y_train)\n",
    "y_pred_l1 = log_reg_l1.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy with L2 Regularization:\", accuracy_score(y_test, y_pred_l2))\n",
    "print(f\"Accuracy with L1 Regularization:\", accuracy_score(y_test, y_pred_l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients from L1 Regularized model\n",
    "l1_coeffs = log_reg_l1.coef_\n",
    "selected_features = [i for i, coeff in enumerate(l1_coeffs) if abs(coeff) < 1]\n",
    "\n",
    "print(f\"Selected features by L1 regularization: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = X.iloc[:, selected_features[:2]]\n",
    "log_reg_l1_selected = LogisticRegression(regularization=\"L1\")\n",
    "log_reg_l1_selected.fit(X_selected, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of points to classify\n",
    "xx, yy = np.meshgrid(np.linspace(X_selected.iloc[:, 0].min(), X_selected.iloc[:, 0].max(), 100),\n",
    "                     np.linspace(X_selected.iloc[:, 1].min(), X_selected.iloc[:, 1].max(), 100))\n",
    "\n",
    "Z = log_reg_l1_selected.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X_selected.iloc[:, 0], X_selected.iloc[:, 1], c=y, edgecolors='k', marker='o', cmap=plt.cm.coolwarm)\n",
    "plt.title(\"Decision Boundary with L1 Regularization (selected features)\")\n",
    "plt.xlabel(f\"Feature {selected_features[0]+1}\")\n",
    "plt.ylabel(f\"Feature {selected_features[1]+1}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
