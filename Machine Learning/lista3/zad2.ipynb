{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit  # Sigmoid function\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, method=\"gradient_descent\", learning_rate=0.01, max_iter=100):\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.coef_ = None\n",
    "        self.avg_step_time = 0\n",
    "        self.training_time = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "        self.loss_history = []\n",
    "        step_time = []\n",
    "\n",
    "        training_start = time()\n",
    "        for _ in range(self.max_iter):\n",
    "            predictions = expit(X @ self.coef_)\n",
    "            step_start = time()\n",
    "            self._step(X, y, predictions)\n",
    "            step_end = time()\n",
    "            step_time.append(step_end - step_start)\n",
    "            self._update_loss(predictions, y)\n",
    "        training_end = time()\n",
    "\n",
    "        self.training_time = training_end - training_start\n",
    "        self.avg_step_time = np.mean(step_time)\n",
    "\n",
    "    def _step(self, X, y, predictions):\n",
    "        if self.method == \"gradient_descent\":\n",
    "            gradient = X.T @ (y - predictions)\n",
    "            self.coef_ += self.learning_rate * gradient\n",
    "        elif self.method == \"newton_raphson\":\n",
    "            W = np.diag(predictions * (1 - predictions))\n",
    "            Hessian = -(X.T @ W @ X)\n",
    "            gradient = X.T @ (y - predictions)\n",
    "            step = np.linalg.solve(Hessian, gradient)\n",
    "            self.coef_ -= step\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "\n",
    "    def _update_loss(self, predictions, y):\n",
    "        loss = -np.mean(y * np.log(predictions + 1e-9) + (1 - y) * np.log(1 - predictions + 1e-9))\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return expit(X @ self.coef_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, 2)\n",
    "true_boundary = np.random.uniform(-5, 5, size=3)\n",
    "epsilon = 0.1\n",
    "logits = X @ true_boundary[1:] + true_boundary[0]\n",
    "probabilities = expit(logits) + np.random.normal(0, epsilon, size=n_samples)\n",
    "y = (probabilities > 0.5).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gd = LogisticRegression(method=\"gradient_descent\", learning_rate=0.01)\n",
    "model_gd.fit(X_train, y_train)\n",
    "\n",
    "model_nr = LogisticRegression(method=\"newton_raphson\")\n",
    "model_nr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Descent Accuracy:\", accuracy_score(y_test, model_gd.predict(X_test)))\n",
    "print(\"Newton-Raphson Accuracy:\", accuracy_score(y_test, model_nr.predict(X_test)))\n",
    "print(\"True Boundary:\", true_boundary)\n",
    "print(\"Gradient Descent Boundary:\", model_gd.coef_)\n",
    "print(\"Newton-Raphson Boundary:\", model_nr.coef_)\n",
    "print(\"Gradient Descent Training Time:\", model_gd.training_time)\n",
    "print(\"Newton-Raphson Training Time:\", model_nr.training_time)\n",
    "print(\"Gradient Descent Mean Step Time:\", model_gd.avg_step_time)\n",
    "print(\"Newton-Raphson Mean Step Time:\", model_nr.avg_step_time)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(model_gd.loss_history, label='Gradient Descent Loss', color='blue')\n",
    "plt.plot(model_nr.loss_history, label='Newton-Raphson Loss', color='red')\n",
    "plt.title('Loss History Comparison')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
