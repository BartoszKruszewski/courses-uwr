{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOmv5opcP3Q0"
      },
      "source": [
        "# Assignment 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z7_EABGg0C7h"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pgMdpxq9z37"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: MLE computation\n",
        "\n",
        "As you know, we can think about linear regression from the optimization point of view (as the problem of minimizing the root mean squared error), but we can also consider it from the probabilistic point of view.\n",
        "Let's consider the following equation:\n",
        "\n",
        "\\begin{equation*}\n",
        "y_i=x_i B_0+\\varepsilon_i\n",
        "\\end{equation*}\n",
        "\n",
        "where $y_i$ is the dependent variable, $x_i$ is a $1 \\times K$ vector of regressors, $\\beta_0$ is the $K \\times 1$ vector of regression coefficients to be estimated and $\\varepsilon_i$ is an unobservable error term.\n",
        "\n",
        "The sample is made up of $N$ IID observations $\\left(y_i, x_i\\right)$.\n",
        "The regression equations can be also written as\n",
        "\n",
        "\\begin{equation*}\n",
        "y=X \\beta_0+\\varepsilon\n",
        "\\end{equation*}\n",
        "\n",
        "where the $N \\times 1$ vector of observations of the dependent variable is denoted by $y$, the $N \\times K$ matrix of regressors is denoted by $X$, and the $N \\times 1$ vector of error terms is denoted by $\\varepsilon$.\n",
        "\n",
        "As everybody interested in probability theory knows, we always have to have some assumptions. In this case, we assume that the vector of errors $\\varepsilon$ has a multivariate normal distribution conditional on $x$, with mean equal to 0 and covariance matrix equal to\n",
        "\n",
        "\\begin{equation*}\n",
        "\\sigma_0^2 I\n",
        "\\end{equation*}\n",
        "\n",
        "where $l$ is the $N \\times N$ identity matrix and\n",
        "\n",
        "\\begin{equation*}\n",
        "\\sigma_0^2=\\operatorname{Var}\\left[\\varepsilon_i \\mid X\\right]\n",
        "\\end{equation*}\n",
        "\n",
        "is the second parameter to be estimated.\n",
        "Furthermore, it is assumed that the matrix of regressors $x$ has full-rank.\n",
        "\n",
        "Those assumptions have interesting implications:\n",
        "- the covariance matrix of $\\varepsilon$ is diagonal implies that the entries of $\\varepsilon$ are mutually independent (i.e., $\\varepsilon_i$ is independent of $\\varepsilon_j$ for $i \\neq j$.)\n",
        "- they all have a normal distribution with mean 0 and variance $\\sigma_0^2$.\n",
        "\n",
        "The fact that we transform normal random variables linearly makes the dependent variable $y_i$ conditionally normal, with mean $x_i \\beta_0$ and variance $\\sigma_0^2$. Therefore, its cdf is\n",
        "\n",
        "\\begin{equation*}\n",
        "f_Y\\left(y_i \\mid X\\right)=\\left(2 \\pi \\sigma_0^2\\right)^{-1 / 2} \\exp \\left(-\\frac{1}{2} \\frac{\\left(y_i-x_i \\beta_0\\right)^2}{\\sigma_0^2}\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "Having this probability model we can use it to do estimation of regression parameters using Maximum Likelihood Estimation method.\n",
        "\n",
        "The first-order conditions for a maximum are\n",
        "\n",
        "\\begin{equation*}\n",
        "\\begin{aligned}\n",
        "& \\nabla_\\beta l\\left(\\beta, \\sigma^2 ; y, X\\right)=0 \\\\\n",
        "& \\frac{\\partial}{\\partial \\sigma^2} l\\left(\\beta, \\sigma^2 ; y, X\\right)=0\n",
        "\\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "where $\\nabla_\\beta$ indicates the gradient calculated with respect to $\\beta$, that is, the vector of the partial derivatives of the log-likelihood with respect to the entries of $\\beta$. The gradient is\n",
        "\n",
        "\\begin{equation*}\n",
        "\\begin{aligned}\n",
        "& \\nabla_\\beta l\\left(\\beta, \\sigma^2: y, X\\right) \\\\\n",
        "= & \\nabla_\\beta\\left(-\\frac{N}{2} \\ln (2 \\pi)-\\frac{N}{2} \\ln \\left(\\sigma^2\\right)-\\frac{1}{2 \\sigma^2} \\sum_{i=1}^N\\left(y_i-x_i \\beta\\right)^2\\right) \\\\\n",
        "= & \\frac{1}{\\sigma^2} \\sum_{i=1}^N x_i^{\\top}\\left(y_i-x_i \\beta\\right) \\\\\n",
        "= & \\frac{1}{\\sigma^2}\\left(\\sum_{i=1}^N x_i^{\\top} y_i-\\sum_{i=1}^N x_i^{\\top} x_i \\beta\\right)\n",
        "\\end{aligned}\n",
        "\\end{equation*}\n",
        "\n",
        "which is equal to zero only if\n",
        "\n",
        "\\begin{equation*}\n",
        "\\sum_{i=1}^N x_i^{\\top} y_i-\\sum_{i=1}^N x_i^{\\top} x_i \\beta=0\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Therefore, the first of the two equations is satisfied if\n",
        "\n",
        "\\begin{equation*}\n",
        "\\beta=\\left(\\sum_{i=1}^N x_i^{\\top} x_i\\right)^{-1} \\sum_{i=1}^N x_i^{\\top} y_i=\\left(X^{\\top} X\\right)^{-1} X^{\\top} y\n",
        "\\end{equation*}\n",
        "\n",
        "where we have used the assumption that $x$ has full rank and, as a consequence, $x^x x$ is invertible.\n",
        "\n",
        "\n",
        "### Subtask A:\n",
        "Prove that the MLE for variance is equal to\n",
        "$$\n",
        "\\widehat{\\sigma}_n^2=\\frac{1}{N} \\sum_{i=1}^N\\left(y_i-x_i \\widehat{\\beta}_N\\right)^2\n",
        "$$\n",
        "\n",
        "### MLE for Logistic regression\n",
        "\n",
        "In the logistic regression model, the output variable $y_i$ is a Bernoulli random variable (it can take only two values, either 1 or 0 ) and\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathrm{P}\\left(y_i=1 \\mid x_i\\right)=S\\left(x_i \\beta\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{equation*}\n",
        "S(t)=\\frac{1}{1+\\exp (-t)}\n",
        "\\end{equation*}\n",
        "\n",
        "is the logistic function, $x_i$ is a $1 \\times K$ vector of inputs and $\\beta$ is a $K \\times 1$ vector of coefficients.\n",
        "\n",
        "Furthermore,\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathrm{P}\\left(y_i=0 \\mid x_i\\right)=1-S\\left(x_i \\beta\\right)\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The vector of coefficients $\\beta$ is the parameter to be estimated by maximum likelihood.\n",
        "We assume that the estimation is carried out with an IID sample comprising $N$ data points\n",
        "\n",
        "\\begin{equation*}\n",
        "\\left(y_i, x_i\\right) \\text { for } i=1, \\ldots, N\n",
        "\\end{equation*}\n",
        "\n",
        "### Subtask B\n",
        "1. Find the form of the loglikelihood.\n",
        "2. Compute the gradient of logistic function with respect to $\\beta$\n",
        "\n",
        "You can either tex your solution and put it in this notebook or attach photos of your solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOZSOSyN6ii7"
      },
      "source": [
        "## Task 2: Implement Logistic Regression\n",
        "\n",
        "Use derivations form previous task to perform logistic regression. Provide a custom implemntation of gradient descent for this taks.\n",
        "**Bonus points** provide also implementation of other optimization algorithm of your choice, eg. Newton-Rhapson method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "touyebTEP6XX"
      },
      "source": [
        "## Task 3: Classification for Imbalanced Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84y1tFTV1jB1"
      },
      "source": [
        "Split the data using a technique suitable for imbalanced classes. Describe its working.\n",
        "\n",
        "Train a logistic regression model on the below data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "fiJxoLCL5KfQ",
        "outputId": "bed28316-e87d-4bea-ef90-7e4e97a204d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_informative=4, n_redundant=5, n_features=20, n_clusters_per_class=3,\n",
        "    n_classes=2,\n",
        "    weights=[0.95, 0.05],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Visualize class distribution\n",
        "def visualize_class_distribution(y):\n",
        "    plt.bar(['Class 0', 'Class 1'], [sum(y == 0), sum(y == 1)])\n",
        "    plt.title(\"Class Distribution\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_class_distribution(y)\n",
        "\n",
        "# Summarize dataset\n",
        "print(\"Dataset Shape:\", X.shape)\n",
        "print(\"Class Distribution:\", np.bincount(y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQWeugoC0F4E"
      },
      "source": [
        "Evaluate your binary classifiers using the following:\n",
        "- confusion matrix,\n",
        "- accuracy,\n",
        "- precision,\n",
        "- recall,\n",
        "- F1 score,\n",
        "- ROCAUC.\n",
        "\n",
        "Please write custom functions calculating the above functions and plotting the receiver operating characteristic curve. Describe your intuition behind each metric and its suitability for imbalanced classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHS8_Q1m2B9e"
      },
      "source": [
        "Propose, describe and implement a technique for improving the model's ability to distinguish the classes. Evaluate your approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUPAfVypQE6K"
      },
      "source": [
        "## Task 4: Optimization of Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba5dpYpYmaJK"
      },
      "source": [
        "The **backpropagation algorithm** is the foundational algorithm in deep learning. In this task You will have to go through a single iteration of said algorithm and compare the results after updating networks weights. <br>\n",
        "You are given a neural network with one input layer consisting of two input nodes ${n_1, n_2}$, one hidden layer with two neurons ${n_3, n_4}$ and an output layer with a single node $n_5$. We will denote the weight connecting $n_i$ to $n_j$ as $w_{i,j}$. A bias related to $n_i$ will be denoted as $b_i$. Parameters are initialized as follows:\n",
        "* $w_{1,3} = 1.5$ ; $w_{2,3} = -2.5$ ; $b_{3} = 0.3$\n",
        "* $w_{1,4} = 1$ ; $w_{2,4} = -2.5$ ; $b_{4} = 0.2$\n",
        "* $w_{3,5} = 4$ ; $w_{4,5} = 3$ ; $b_{5} = -0.8$ <br>\n",
        "\n",
        "Additionally, neurons $n_3$ and $n_4$ are equipped with the sigmoid activation function: $\\sigma(x) = \\frac{1}{1+e^{-x}} $<br>\n",
        "Your task is to use two training examples: $x = \\{ (1,2), (2,0)\\}$, $y = \\{-1, 6\\}$ to preform a single backpropagation step - preform forward calculations and propagate the received error backwards, updating the weights. After all weights have been updated, check how the predictions change, preforming additional forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "820zmQ82QL26"
      },
      "source": [
        "## Task 5: L1 & L2 Regularization for Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k0lFAXLZBfO"
      },
      "source": [
        "In this task we will use a synthetic dataset generated by the function `make_classification`.\n",
        "\n",
        "1. Use a pairplot to see the relationships between the features in the dataset.\n",
        "2. Apply your logistic regression model from Task 1 on this dataset and measure its accuracy.\n",
        "3. Modify your implementation to account for two regularization methods: L1 and L2 and apply them to the dataset. What can you observe?\n",
        "4. Which features are selected by L1 regularization? Plot the decision boundary for these features.\n",
        "\n",
        "(If you have not done Task 1, use `LogisticRegression` from `sklearn` to see the effects of regularization. You can get half of the points for this task if you do that.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-refGL3PQRvn"
      },
      "outputs": [],
      "source": [
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=5,\n",
        "    n_redundant=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dataset = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(1, 21)])\n",
        "dataset['Target'] = y"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
